{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5d5625-948f-4822-a7fa-6542de342937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.runnables import RunnableSequence, RunnableBranch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f2f3604-e7d6-447d-8d90-83fe6e2a7d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9672321d-306b-4811-bea0-9f3eeecb38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "identification_template = ChatPromptTemplate([\n",
    "    (\"system\", \"you are an chat classifier to see if the user asks question about sql or general query return only True/False\"),\n",
    "    (\"human\", \"please classify if my question: {question} is about asking sql query or general\")\n",
    "])\n",
    "identification_chain = RunnableSequence(identification_template, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50273dd9-ee53-4bd1-927a-af6bfa657eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_template = ChatPromptTemplate([\n",
    "    (\"system\", \"you are an helpful AI agent answers user questions.\"),\n",
    "    (\"human\", \"please classify doubt: {question}\")\n",
    "])\n",
    "general_chain = RunnableSequence(general_template, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349541b-d49b-4943-b603-8728c804cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunnableBranch(\n",
    "    (lambda x: isinstance(x, str), lambda x: x.upper()),\n",
    "    (lambda x: isinstance(x, int), lambda x: x + 1),\n",
    "    (lambda x: isinstance(x, float), lambda x: x * 2),\n",
    "    lambda x: \"goodbye\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03561b6c-7a08-4f51-9dc2-fd496060fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ed393-f4b1-4695-8664-e2f800fd4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_node(\"llm\", chat)\n",
    "workflow.add_edge(START, \"llm\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
